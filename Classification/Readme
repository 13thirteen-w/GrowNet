######## ReadMe file for GrowNet on Classification ########

########## General Info: ##############

1. GrowNet Backup file is general file containing all codes.  All classification related codes are in Classification file : GrowNet Backup/Classification/

2. All classification datasets are located in server: gputest025-pa4.central.criteo.preprod /var/opt/data/user_data/s.badirli/:

# dataset                # Number of features

a9a                            123
ijcnn1                         22
real-sim                       20958
Criteo_Dracula                 766
Higgs                          28


3. Data loading and creating dataloader are handled in GrowNet Backup/Classification/data/data.py . If you have a sparse data then try using sparse_data.py and opt sparse to True 

4. Individual model class and ensemble architecture are in GrowNet Backup/Classification/models:  mlp.py and dynamic_net.py. 
You can increase number of hidden layers or activation function from there: mlp.py

5. All results together with hyper parameters are stored in GrowNet Backup/Classification/results folder in .npz file.

6. In each stage Boosting Net is saved to GrowNet Backup/Classification/ckpt folder. 

7. GrowNet Backup/Classification/main_cls_cv.py is the main script for classification task where the number of models are selected based on CV (5-10% of the training data)
are reserved for validation and tested on the Test data. You may change the normalization technique or optimizer from there.

8. GrowNet Backup/Classification/train_cls.sh is the shell script that runs the code where you can also play around hyperparameters of the GBNN:
Parameters: (Sample example)

data =Higgs  				                       --> name of the dataset

CUDA_VISIBLE_DEVICES=1 python main_cls_cv.py \                 --> main python script to run. You may run main_reg.py but get rid of --cv parameter.
    --feat_d 28 \                                              --> Int: # of features in the data: shell script itself has the numbers commented in
    --hidden_d 16 \                                            --> Int: # of units in the hidden layer. I generally choose it between feat_d/4 and feat_d*3/4
    --boost_rate 1 \					       --> Float: Boost rate to weight the model outputs for final output
    --lr 0.005 \					       --> Float: Learning rate	
    --L2 .0e-3 \					       --> Float: L2 regularization rate (weight decay)	
    --num_nets 40 \					       --> Int: Max number of models in the net	 
    --data ${data} \					       --> Assigning data from above	
    --tr /var/opt/data/user_data/s.badirli/${data}_tr.npz \    --> Path to the training data 
    --te /var/opt/data/user_data/s.badirli/${data}_te.npz \    --> Path to the test data
    --batch_size 2048 \					       --> Int: Training batch size	
    --epochs_per_stage 1 \				       --> Int: Number of epochs for training each model: generally set to 1 sometimes 2
    --correct_epoch 1 \					       --> Int: Number of epochs for the Corrective step: generally set to 1 or 2
    --normalization True \				       --> Boolean: Normalization, in regression, standardization to apply or Not
    --cv True \						       --> Boolean: Performing CV to select number of models in the Boosting, reserving 5-10% of the training data. You can change the percentage inside of 								   main script: get_data()
    --sparse False \					       --> To help training if the data is sparse set this parameter to True
    --out_f ./ckpt/${data}_reg.pth \			       --> Saving the model in each stage (new model added) to this file 
    --cuda 						       --> Cuda option: On our case it is ON



########## How top Run the Code ##############

1. Change your directory to GrowNet Backup/Classification/

2. Open train_cls.sh shell script (vim train_cls.sh) and follow these steps:
	
	a. Choose the data name you want to try: data = Higgs
	
	b. Change --feat_d into the number of features the dataset has: in the case of Higgs  it is 28
	
	c. You may want to adjust the number of units in hidden layer based on the dataset: I generally choose the hidden_d something between  feat_d/4 and 3*feat_d/4
	If the hidden_d is very big then models overfits otherwise if very small can't learn.

	d. If you want to try  new Classification data then be sure you put it into .npz format with the name analogous to --tr and --te. 
	   For now please also manually  add the data name into manuscript in the get_data() function.

	e. The rest is optional: you may or may not change 

3. To enforce the changes you have done run this line on command line: chmod +x train_cls.sh

4. Finally run the script by executing this line on command window: ./train_cls.sh


